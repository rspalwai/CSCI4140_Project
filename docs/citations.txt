Schulman, John, et al. "Proximal Policy Optimization Algorithms." arXiv, 28 Aug. 2017, https://arxiv.org/pdf/1707.06347.

Xu, Shusheng, et al. "Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study." arXiv, 10 Oct. 2024, https://arxiv.org/abs/2404.10719.

Wang, Zhichao, et al. "A Comprehensive Survey of LLM Alignment Techniques: RLHF, RLAIF, PPO, DPO and More." arXiv, 23 July 2024, https://arxiv.org/abs/2407.16216.

Kaufmann, Timo, et al. "A Survey of Reinforcement Learning from Human Feedback." arXiv, 30 Apr. 2024, https://arxiv.org/abs/2312.14925.

OpenAI. "Welcome to Spinning Up in Deep RL!" OpenAI Spinning Up Documentation, https://spinningup.openai.com/.

Hugging Face. "TRL - Transformer Reinforcement Learning." Hugging Face Documentation, https://huggingface.co/docs/trl/index.

"Reinforcement Learning from Human Feedback." Wikipedia, 30 Nov. 2024, https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback.

Ouyang, Long, et al. "Training Language Models to Follow Instructions with Human Feedback." arXiv, 30 Mar. 2022, https://arxiv.org/abs/2203.02155.
